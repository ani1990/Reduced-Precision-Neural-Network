{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BNN USING lARQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(100)\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(0)\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "import keras\n",
    "from keras.layers import Activation\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.optimizers import SGD, Adam, RMSprop\n",
    "from quantized.layers import QuantConv2D,QuantDense\n",
    "from quantized.models import summary\n",
    "from quantized.math import binary_tanh as binary_ops\n",
    "from quantized.quantizers import ste_sign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading customized activation function\n",
    "def binary_tanh(x):\n",
    "    return binary_ops(x)\n",
    "\n",
    "#loading and preprocessing data from dataset\n",
    "#please change path as per dataset in your system\n",
    "trdata = ImageDataGenerator()\n",
    "\n",
    "traindata = trdata.flow_from_directory(directory=\"/training_set/training_set\",target_size=(227,227))\n",
    "trdata = ImageDataGenerator()\n",
    "\n",
    "testdata = trdata.flow_from_directory(directory=\"/test_set/test_set\",target_size=(227,227))\n",
    "\n",
    "#kwargs: to be passed to layer of network to binarize the value and operation\n",
    "\n",
    "kwargs = dict(input_quantizer=\"ste_sign\",\n",
    "              kernel_quantizer=\"ste_sign\",\n",
    "              kernel_constraint=\"weight_clip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining and creating the model as per architecture\n",
    "model = Sequential()\n",
    "\n",
    "#model.add(Conv2D(256, (3, 3), input_shape=X.shape[1:]))\n",
    "\n",
    "model.add(QuantConv2D(96, (11, 11),\n",
    "                                kernel_quantizer=\"ste_sign\",\n",
    "                                kernel_constraint=\"weight_clip\",\n",
    "                                use_bias=False,strides=4, padding=\"valid\",\n",
    "                                input_shape=(227,227,3),activation = \"relu\"))\n",
    "#model.add(tf.keras.layers.Activation(\"relu\"))\n",
    "# Max Pooling\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(3,3), strides=(2,2), padding=\"valid\"))\n",
    "\n",
    "# 2nd Convolutional Layer\n",
    "model.add(QuantConv2D(256, (5, 5),**kwargs,\n",
    "                                use_bias=False,data_format='channels_last',\n",
    "                                strides=(1,1), padding=\"same\"))\n",
    "model.add(tf.keras.layers.Activation(binary_tanh))\n",
    "\n",
    "# Max Pooling\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(3,3), strides=(2,2), padding=\"valid\"))\n",
    "\n",
    "# 3rd Convolutional Layer\n",
    "model.add(QuantConv2D(384, (3, 3),**kwargs,\n",
    "                                use_bias=False,data_format='channels_last',\n",
    "                                strides=(1,1), padding=\"same\"))\n",
    "model.add(tf.keras.layers.Activation(binary_tanh))\n",
    "\n",
    "# 4th Convolutional Layer\n",
    "model.add(QuantConv2D(384, (3, 3),**kwargs,\n",
    "                                use_bias=False,data_format='channels_last',\n",
    "                                strides=(1,1), padding=\"same\"))\n",
    "model.add(tf.keras.layers.Activation(binary_tanh))\n",
    "\n",
    "# 5th Convolutional Layer\n",
    "model.add(QuantConv2D(256, (3, 3),**kwargs,\n",
    "                                use_bias=False,data_format='channels_last',\n",
    "                                strides=(1,1), padding=\"same\"))\n",
    "\n",
    "model.add(tf.keras.layers.Activation(binary_tanh))\n",
    "\n",
    "# Max Pooling\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(3,3), strides=(2,2), padding=\"valid\"))\n",
    "\n",
    "# Passing it to a Fully Connected layer\n",
    "model.add(Flatten())\n",
    "# 1st Fully Connected Layer\n",
    "model.add(QuantDense(9216, **kwargs))\n",
    "model.add(tf.keras.layers.Activation(binary_tanh))\n",
    "\n",
    "# 2nd Fully Connected Layer\n",
    "model.add(QuantDense(4096, **kwargs))\n",
    "model.add(tf.keras.layers.Activation(binary_tanh))\n",
    "\n",
    "# 3rd Fully Connected Layer\n",
    "model.add(QuantDense(4096, **kwargs))\n",
    "model.add(tf.keras.layers.Activation(binary_tanh))\n",
    "\n",
    "# Output Layer\n",
    "model.add(QuantDense(2, use_bias=False, **kwargs)) #As we have two classes\n",
    "model.add(tf.keras.layers.Activation(\"softmax\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(lr=0.001)\n",
    "model.compile(optimizer=opt, loss=keras.losses.categorical_crossentropy, metrics=['accuracy'])\n",
    "model.summary()\n",
    "summary(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining Earlystopping callback and saving the best model for future use\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"alexnet_fp_bnn.h5\", monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "early = EarlyStopping(monitor='val_acc', min_delta=0, patience=20, verbose=1, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training and validating train and validation data respectively\n",
    "hist = model.fit_generator(steps_per_epoch=2,generator=traindata, validation_data= testdata, \n",
    "                           validation_steps=10,epochs=40,callbacks=[checkpoint,early])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving model for future use\n",
    "model.save(\"model_alexnet_mnist_bnn_bit.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graph represenattaion of loss and accuracy\n",
    "plt.figure()\n",
    "plt.plot(hist.history[\"acc\"])\n",
    "plt.plot(hist.history[\"val_acc\"])\n",
    "plt.title('BNN model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'Validation'], loc='lower right')\n",
    "plt.figure()\n",
    "print(f\"Training maximum accuracy: {np.max(hist.history['acc']) * 100:.2f} %\")\n",
    "print(f\"Validation maximum accuracy: {np.max(hist.history['val_acc']) * 100:.2f} %\")\n",
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['val_loss'])\n",
    "plt.title('BNN model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train_loss', 'Validation_loss'], loc='upper right')\n",
    "\n",
    "print(f\"Training model minimum loss: {np.min(hist.history['loss'])}\")\n",
    "print(f\"Validation model minimum loss : {np.min(hist.history['val_loss']) }\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
